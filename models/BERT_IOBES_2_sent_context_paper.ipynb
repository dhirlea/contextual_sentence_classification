{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ex_4NNmtMT2",
        "outputId": "5ce532e1-79aa-4a24-bedd-14ccb6b741e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.11.2-py3-none-any.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 2.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 39.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |█████████████████████▌          | 2.2 MB 54.9 MB/s eta 0:00:01"
          ]
        }
      ],
      "source": [
        "# DELETE CELL IF RUNNING ON LOCAL MACHINE INSTEAD OF GOOGLE COLAB\n",
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install pytorch-crf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Utm8lRFa7Tdt"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torchcrf import CRF\n",
        "from torch.utils.data import  DataLoader\n",
        "\n",
        "import transformers\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from transformers import BertTokenizer \n",
        "from transformers import BertPreTrainedModel, BertModel\n",
        "\n",
        "from datasets import load_metric\n",
        "\n",
        "import numpy as np\n",
        "import re\n",
        "import json\n",
        "import os\n",
        "import time\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "from tqdm.auto import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m9kckp85tQGY"
      },
      "outputs": [],
      "source": [
        "# DELETE CELL IF RUNNING ON LOCAL MACHINE INSTEAD OF GOOGLE COLAB\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qgAmDyAOtSj9"
      },
      "outputs": [],
      "source": [
        "# DELETE CELL IF RUNNING ON LOCAL MACHINE INSTEAD OF GOOGLE COLAB\n",
        "%cd /content/drive/MyDrive "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chQJjqSw7Tdu"
      },
      "outputs": [],
      "source": [
        "# Setting random seed and device\n",
        "SEED = 1\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "print(device)\n",
        "print(use_cuda)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TR1-pIFXtB9t"
      },
      "outputs": [],
      "source": [
        "# Create list of training data files\n",
        "def load_from_directory(directory):\n",
        "    \"\"\"\n",
        "    Utility function to load all json-converted reports into a dataset.\n",
        "    params: directory: string representing location on disk of json files\n",
        "    returns: dataset: list of deserialised jsons\n",
        "    \"\"\"\n",
        "    path = os.getcwd()\n",
        "    path = os.path.join(path, directory)\n",
        "    json_files = [pos_json for pos_json in os.listdir(path) if pos_json.endswith('.json')]\n",
        "\n",
        "    dataset = [] \n",
        "\n",
        "    for filename in json_files: \n",
        "        with open(path+filename, \"r\", encoding='utf-8') as read_file:\n",
        "            dataset.append(json.load(read_file))\n",
        "    \n",
        "    return dataset\n",
        "    \n",
        "train_folder = 'json_train/'\n",
        "train_dataset = load_from_directory(train_folder)\n",
        "\n",
        "dev_folder = 'json_develop/'\n",
        "development_dataset = load_from_directory(dev_folder)\n",
        "\n",
        "test_folder = 'json_test/'\n",
        "test_dataset = load_from_directory(test_folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xG-4ESe7tB9u"
      },
      "outputs": [],
      "source": [
        "def filter_dataset(input_dataset):\n",
        "    \"\"\" \n",
        "    Utility function to convert input dataset into custom data structure\n",
        "    params: input_dataset: list of deserialised jsons\n",
        "    returns: dictionary with the following structure:\n",
        "            key:  sentence_global_idx value , value:list of dictionaries\n",
        "            dict0: key: report_no, value: int \n",
        "            dict1: key: text, value: string\n",
        "            dict2: key: has_initiative, value: boolean\n",
        "            dict3: key: list_of_initiatives, value: list of strings with initiative IDs\n",
        "            dict4: key: sector, value: list of strings\n",
        "            dict5: key: sdg, value: list of sgd strings (representing sgd number)\n",
        "            dict6: key: sentence_length, value: int  \n",
        "    \"\"\"\n",
        "    structured_data = {}\n",
        "    total_no_reports = len(input_dataset)\n",
        "    sentence_global_idx = 0\n",
        "    re_punctuation_string = '[“”|()%&\\s,_:;/\\'!?-]'\n",
        "\n",
        "    for report_no in range(total_no_reports): \n",
        "        no_sentences_per_report = len(input_dataset[report_no]['tokenised_sentences'])\n",
        "        for sentence_no in range(no_sentences_per_report):\n",
        "            tokenized_sentence = re.split(re_punctuation_string, input_dataset[report_no]['tokenised_sentences'][sentence_no]['text'])\n",
        "            tokenized_sentence = list(filter(None, tokenized_sentence))\n",
        "            if (len(tokenized_sentence) == 0):           \n",
        "                continue\n",
        "            else:\n",
        "                structured_data[sentence_global_idx] = []\n",
        "                structured_data[sentence_global_idx].append({'report_no':report_no}) \n",
        "                structured_data[sentence_global_idx].append({'text':' '.join([elem.lower() for elem in tokenized_sentence])})\n",
        "                if len(input_dataset[report_no]['tokenised_sentences'][sentence_no]['initiative_ids']) > 0:\n",
        "                  structured_data[sentence_global_idx].append({'has_initiative':1})\n",
        "                else:\n",
        "                   structured_data[sentence_global_idx].append({'has_initiative':0})\n",
        "                structured_data[sentence_global_idx].append({'list_of_initiatives': input_dataset[report_no]['tokenised_sentences'][sentence_no]['initiative_ids']}) \n",
        "                \n",
        "                structured_data[sentence_global_idx].append({'sentence_length':len(tokenized_sentence)}) \n",
        "                sentence_global_idx +=1\n",
        "    \n",
        "    return structured_data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rqb_rDqdtB9w"
      },
      "outputs": [],
      "source": [
        "# Set up datasets from json files\n",
        "training_data = filter_dataset(train_dataset)\n",
        "development_data = filter_dataset(development_dataset)\n",
        "testing_data = filter_dataset(test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uJ2h2bUHtB9y"
      },
      "outputs": [],
      "source": [
        "def assisted_labelling(data, lower_threshold, upper_threshold):\n",
        "    \"\"\"\n",
        "    Utility function which labels all sentences with fewer than the threshold number of tokens as not having a sustainability initiative.\n",
        "    params: data: list of dictionaries\n",
        "            threshold: int representing number of tokens\n",
        "    returns: dictionary {global_sentence_index:boolean label}\n",
        "    \"\"\"\n",
        "    labeled_dataset = {}\n",
        "    for sentence_no in range(len(data)):\n",
        "        tokenized_sentence = re.split(' ', data[sentence_no][1]['text'])\n",
        "        tokenized_sentence_with_alphabetical_chars = [word for word in tokenized_sentence if re.search('[a-zA-Z]', word)]\n",
        "        if (data[sentence_no][4]['sentence_length'] > lower_threshold) & (len(tokenized_sentence_with_alphabetical_chars)!=0) & (data[sentence_no][4]['sentence_length']<upper_threshold):\n",
        "            labeled_dataset[sentence_no] = 1 \n",
        "        else:\n",
        "            labeled_dataset[sentence_no] = 0 # label short, long and non-alphabetical sentences as not having an initiative\n",
        "    return labeled_dataset\n",
        "\n",
        "assistant_labeled_training_data = assisted_labelling(training_data,lower_threshold=5, upper_threshold=100)\n",
        "assistant_labeled_dev_data = assisted_labelling(development_data,lower_threshold=5, upper_threshold=100)\n",
        "assistant_labeled_test_data = assisted_labelling(testing_data, lower_threshold=5, upper_threshold=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lq_Q9UCl33gd"
      },
      "outputs": [],
      "source": [
        "def reader(dataset, assisted_labels, initiative_dict):\n",
        "    \"\"\"\n",
        "    Utility function read in the data together with assisted labels and return a train dictionary and a pre-labelled dictionary.\n",
        "    params: dataset: dict {global_sentence_index : list of 7 dictionaries}\n",
        "            assisted_labels: dict {global_sentence_index : assistant label}\n",
        "    returns: train_dict: dict\n",
        "             pre_labelled_dict\n",
        "    \"\"\"\n",
        "    texts = []\n",
        "    labels = []\n",
        "    positions = []\n",
        "    initiative_IDs = []\n",
        "    report_no = []\n",
        "    \n",
        "    pre_labeled_texts =[]\n",
        "    pre_labeled_labels =[]\n",
        "    pre_labeled_positions =[]\n",
        "    pre_labeled_IDs = []\n",
        "\n",
        "    for sentence_no in range(len(dataset)):\n",
        "        if assisted_labels[sentence_no] == 1:\n",
        "            report_no.append(dataset[sentence_no][0]['report_no'])\n",
        "            texts.append(dataset[sentence_no][1]['text'])\n",
        "            if dataset[sentence_no][3]['list_of_initiatives']: #check whether the sentence has an initiative\n",
        "              initiative_unique_reference = dataset[sentence_no][3]['list_of_initiatives'][0] + '_' + str(dataset[sentence_no][0]['report_no'])\n",
        "              if len(initiative_dict[initiative_unique_reference]) == 1:\n",
        "                labels.append(dataset[sentence_no][2]['has_initiative']) # append 1 for singletons or 0 for non-initiative sentences\n",
        "              elif initiative_dict[initiative_unique_reference].index(sentence_no) == 0:\n",
        "                labels.append(2) #append 2 for beginning of initiative\n",
        "              elif initiative_dict[initiative_unique_reference].index(sentence_no) == (len(initiative_dict[initiative_unique_reference]) - 1):\n",
        "                labels.append(4) #append 4 for end of initiative\n",
        "              else:\n",
        "                labels.append(3) #append 3 for inside an initiative\n",
        "            else:\n",
        "              labels.append(dataset[sentence_no][2]['has_initiative'])\n",
        "            positions.append(sentence_no)\n",
        "            initiative_IDs.append(dataset[sentence_no][3]['list_of_initiatives'])\n",
        "        else:\n",
        "            pre_labeled_texts.append(dataset[sentence_no][1]['text'])\n",
        "            pre_labeled_labels.append(assisted_labels[sentence_no]) # append 0 for non-initiative sentences\n",
        "            pre_labeled_positions.append(sentence_no)\n",
        "            pre_labeled_IDs.append(dataset[sentence_no][3]['list_of_initiatives'])\n",
        "\n",
        "    actual_data_dict = {'texts':texts, 'labels':labels, 'positions':positions, 'ID_list':initiative_IDs, 'report_no':report_no}\n",
        "    pre_labeled_dict = {'texts':pre_labeled_texts, 'labels':pre_labeled_labels, 'positions': pre_labeled_positions, 'ID_list':pre_labeled_IDs}\n",
        "            \n",
        "    return actual_data_dict, pre_labeled_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FrxI7zhZzd_I"
      },
      "outputs": [],
      "source": [
        "def context_builder(data_dict, left_context_size = 0, right_context_size = 0):\n",
        "    \"\"\" Utility function to build context around a target sentence.\n",
        "    \"\"\"\n",
        "    context = []\n",
        "    multi_sentence_labels = []\n",
        "    for sentence_index in range(len(data_dict['texts'])):\n",
        "        sentence_context = []\n",
        "        sentence_context_labels = []\n",
        "        if (sentence_index - left_context_size >= 0) and (sentence_index + right_context_size < len(data_dict['texts'])):\n",
        "            # test if target sentence is in the middle of the corpus\n",
        "            for context_index in range(sentence_index - left_context_size, sentence_index + right_context_size + 1):\n",
        "                if data_dict['report_no'][sentence_index] == data_dict['report_no'][context_index]:\n",
        "                    sentence_context.append(data_dict['texts'][context_index])\n",
        "                    sentence_context_labels.append(data_dict['labels'][context_index])\n",
        "        elif sentence_index - left_context_size >= 0: #if target sentence is at end of the corpus \n",
        "            for context_index in range(sentence_index - left_context_size, sentence_index + right_context_size + 1):\n",
        "                if context_index < len(data_dict['texts']): # add in a smaller context window at end of the corpus\n",
        "                    if (data_dict['report_no'][sentence_index] == data_dict['report_no'][context_index]):\n",
        "                        sentence_context.append(data_dict['texts'][context_index])\n",
        "                        sentence_context_labels.append(data_dict['labels'][context_index])\n",
        "        elif sentence_index + right_context_size < len(data_dict['texts']): #if target sentence is at beginning of the corpus \n",
        "                for context_index in range(sentence_index - left_context_size, sentence_index + right_context_size + 1):\n",
        "                    if context_index >= 0: # add in smaller context window at the beginning of the corpus\n",
        "                        if (data_dict['report_no'][sentence_index] == data_dict['report_no'][context_index]):\n",
        "                            sentence_context.append(data_dict['texts'][context_index])\n",
        "                            sentence_context_labels.append(data_dict['labels'][context_index])\n",
        "        context.append(sentence_context)\n",
        "        while len(sentence_context_labels) < (1 + left_context_size + right_context_size): # pad with 0 labels for senteces with a smaller context eg. beginning/end of docs\n",
        "          sentence_context_labels.append(0)\n",
        "        multi_sentence_labels.append(sentence_context_labels)\n",
        "    return context, multi_sentence_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9D9lIa3R6cRb"
      },
      "outputs": [],
      "source": [
        "class SustainableDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"Dataset class inheriting from pytorch to be used by dataloaders.\n",
        "    \"\"\"\n",
        "    def __init__(self, tokenizer, input_set, input_context, input_multi_sentence_labels, max_paragraph_length, global_target_sentence_index):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.texts = input_set['texts']\n",
        "        self.labels = input_set['labels']\n",
        "        self.report_nos = input_set['report_no']\n",
        "        self.contexts = input_context\n",
        "        self.context_labels = input_multi_sentence_labels\n",
        "        self.max_paragraph_length = max_paragraph_length\n",
        "        self.global_target_sentence_index = global_target_sentence_index\n",
        "        \n",
        "        \n",
        "    def collate_fn(self, batch):\n",
        "        texts = [b['text'] for b in batch]\n",
        "        labels = [b['label'] for b in batch]\n",
        "        contexts = [b['context'] for b in batch]\n",
        "        context_labels = [b['context_label'] for b in batch]\n",
        "        encodings, sep_positions = self.custom_tokenizer(batch = contexts) \n",
        "        encodings['labels'] =  torch.tensor(context_labels) # pass through labels for all sentences\n",
        "        encodings['sep_positions'] = sep_positions\n",
        "        return encodings\n",
        "    \n",
        "    def custom_collate_fn(self, batch):\n",
        "      texts = [b['text'] for b in batch]\n",
        "      labels = [b['label'] for b in batch]\n",
        "      contexts = [b['context'] for b in batch]\n",
        "      context_labels = [b['context_label'] for b in batch]\n",
        "      return {'texts':texts, 'labels':labels, 'contexts':contexts, 'context_labels':context_labels}\n",
        "    \n",
        "\n",
        "    def custom_tokenizer(self, batch):\n",
        "      \"\"\" Utility functions to tokenize a list of sentences using [SEP] at the beginning of each sentence with fixed positions.\n",
        "      \"\"\"\n",
        "      batch_sequences = []\n",
        "      batch_sep_positions = []\n",
        "      batch_token_type_ids = []\n",
        "      for sequence_list in batch:\n",
        "        augmented_sequence = ''\n",
        "        for sentence in sequence_list:\n",
        "            augmented_sequence += '[SEP]' + sentence\n",
        "        augmented_sequence.strip()\n",
        "        batch_sequences.append(augmented_sequence)\n",
        "      encoded_batch = self.tokenizer(batch_sequences, padding='longest', truncation=True, max_length=512, return_tensors='pt')\n",
        "      for encoded_sequence in encoded_batch['input_ids']:\n",
        "          sep_positions = [index for index in range(len(encoded_sequence)) if encoded_sequence[index]==102]\n",
        "          while len(sep_positions) < self.max_paragraph_length + 1: # repeat last sep position to get full sequence \n",
        "            sep_positions.append(sep_positions[-1])\n",
        "          batch_sep_positions.append(sep_positions)\n",
        "          if self.global_target_sentence_index in range(len(sep_positions)-1): # check to see that the target sentence is actually part of the context\n",
        "            custom_token_type_ids = [ 1 if index in range(sep_positions[self.global_target_sentence_index], sep_positions[self.global_target_sentence_index+1]+1) else 0 for index in range(len(encoded_sequence))]\n",
        "          else:\n",
        "            custom_token_type_ids = [1 for index in range(len(encoded_sequence))]\n",
        "          batch_token_type_ids.append(custom_token_type_ids)\n",
        "       \n",
        "      encoded_batch['token_type_ids'] = torch.tensor(batch_token_type_ids)\n",
        "      return encoded_batch, batch_sep_positions\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {'text': self.texts[idx],\n",
        "                'label': self.labels[idx],\n",
        "                'context': self.contexts[idx],\n",
        "                'context_label' : self.context_labels[idx],\n",
        "                }\n",
        "        return item"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U9odLvcadGWg"
      },
      "outputs": [],
      "source": [
        "def get_sep_positions(data_loader, global_target_sentence_index, max_paragraph_length, tokenizer):\n",
        "  \"\"\" Utility function to get sep positions for given dataset.\n",
        "  \"\"\"\n",
        "  dataset_sep_positions = []\n",
        "  for batch in data_loader:\n",
        "      batch_sep_positions = []\n",
        "      batch_sequences = []\n",
        "      for sequence_list in batch['contexts']:\n",
        "        augmented_sequence = ''\n",
        "        for sentence in sequence_list:\n",
        "            augmented_sequence += '[SEP]' + sentence\n",
        "        augmented_sequence.strip()\n",
        "        batch_sequences.append(augmented_sequence)\n",
        "      encoded_batch = tokenizer(batch_sequences, padding='longest', truncation=True, max_length=512, return_tensors='pt')\n",
        "      for encoded_sequence in encoded_batch['input_ids']:\n",
        "          sep_positions = [index for index in range(len(encoded_sequence)) if encoded_sequence[index]==102]\n",
        "          while len(sep_positions) < max_paragraph_length + 1: # repeat last sep position to get full sequence \n",
        "            sep_positions.append(sep_positions[-1])\n",
        "          batch_sep_positions.append(sep_positions)\n",
        "      dataset_sep_positions.extend([sublist[:-1] for sublist in batch_sep_positions]) \n",
        "  return dataset_sep_positions\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r92owFJgsoJV"
      },
      "outputs": [],
      "source": [
        "# Create a dictionary with ID_reportNo as keys and values as list of sentence indices\n",
        "def create_init_dict(data):\n",
        "    \"\"\" Utility function to extract individual initiatives as keys of a dict and a list of corresponding global sentence indices.\n",
        "    \"\"\"\n",
        "    initiative_dict = {} #keys are initiative IDs, values are counts of IDs \n",
        "    for sentence_no in range(len(data)):\n",
        "        if data[sentence_no][2]['has_initiative']:\n",
        "            initiative_ID = data[sentence_no][3]['list_of_initiatives'][0]\n",
        "            if (initiative_ID + '_' + str(data[sentence_no][0]['report_no'])) not in initiative_dict.keys():\n",
        "                initiative_dict[initiative_ID + '_' + str(data[sentence_no][0]['report_no'])] = [sentence_no]\n",
        "            else:\n",
        "                initiative_dict[initiative_ID + '_' + str(data[sentence_no][0]['report_no'])].append(sentence_no)\n",
        "    return initiative_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TyZQtXwj4ixr",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Initialize tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "global_target_sentence_index = 2 #target sentence index to be used throughout entire script\n",
        "\n",
        "# Create gold standard initiative dictionaries\n",
        "train_initiative_dict = create_init_dict(training_data)\n",
        "dev_initiative_dict = create_init_dict(development_data)\n",
        "test_initiative_dict = create_init_dict(testing_data)\n",
        "\n",
        "# Read in training and dev data and split into data to feed into the model and pre-labeled data\n",
        "train_data, pre_labeled_train_data = reader(training_data, assistant_labeled_training_data, train_initiative_dict)\n",
        "dev_data, pre_labeled_dev_data = reader(development_data, assistant_labeled_dev_data, dev_initiative_dict)\n",
        "test_data, pre_labeled_test_data = reader(testing_data, assistant_labeled_test_data, test_initiative_dict)\n",
        "\n",
        "# Construct context around each sentence per dataset\n",
        "train_context, train_multi_sentence_labels = context_builder(train_data, left_context_size = 2, right_context_size = 2) \n",
        "dev_context, dev_multi_sentence_labels = context_builder(dev_data, left_context_size = 2, right_context_size = 2) \n",
        "test_context, test_multi_sentence_labels = context_builder(test_data, left_context_size = 2, right_context_size = 2)\n",
        "\n",
        "max_paragraph_length = max([len(label_sequence) for label_sequence in train_multi_sentence_labels])\n",
        "\n",
        "# Only data to be fed into the model is built into datasets\n",
        "train_dataset = SustainableDataset(tokenizer, train_data, train_context, train_multi_sentence_labels, max_paragraph_length = max_paragraph_length, global_target_sentence_index = global_target_sentence_index)\n",
        "dev_dataset = SustainableDataset(tokenizer, dev_data, dev_context, dev_multi_sentence_labels, max_paragraph_length = max_paragraph_length, global_target_sentence_index = global_target_sentence_index)\n",
        "test_dataset = SustainableDataset(tokenizer, test_data, test_context, test_multi_sentence_labels, max_paragraph_length = max_paragraph_length, global_target_sentence_index = global_target_sentence_index)\n",
        "\n",
        "# Create train and dev dataloaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=False, collate_fn = train_dataset.custom_collate_fn)\n",
        "dev_loader = DataLoader(dev_dataset, batch_size=16, shuffle=False, collate_fn = dev_dataset.custom_collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, collate_fn = test_dataset.custom_collate_fn)\n",
        "\n",
        "# Record sep positions for dev dataset (to be used for early stopping during training)\n",
        "dev_sep_positions = get_sep_positions(dev_loader, global_target_sentence_index = global_target_sentence_index, max_paragraph_length = max_paragraph_length, tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-KwKMBsULGd"
      },
      "outputs": [],
      "source": [
        "# Unit tests for building context assuming a window\n",
        "mock_train_context, mock_train_multi_sentence_labels = context_builder(train_data, left_context_size = 1, right_context_size = 1)\n",
        "mock_dev_context, mock_dev_multi_sentence_labels = context_builder(dev_data, left_context_size = 1, right_context_size = 1)\n",
        "mock_train_dataset = SustainableDataset(tokenizer, train_data, mock_train_context, mock_train_multi_sentence_labels, max_paragraph_length = max_paragraph_length, global_target_sentence_index = global_target_sentence_index)\n",
        "mock_dev_dataset = SustainableDataset(tokenizer, dev_data, mock_dev_context, mock_dev_multi_sentence_labels, max_paragraph_length = max_paragraph_length, global_target_sentence_index = global_target_sentence_index)\n",
        "assert mock_train_dataset.contexts[11] == [mock_train_dataset.texts[10], mock_train_dataset.texts[11], mock_train_dataset.texts[12]]  #random corpus context\n",
        "assert len(mock_train_dataset.contexts) == len(mock_train_dataset.texts) #there is a context for every target sentence in the train set\n",
        "assert mock_dev_dataset.contexts[11] == [mock_dev_dataset.texts[10], mock_dev_dataset.texts[11], dev_dataset.texts[12]] #random corpus context\n",
        "assert len(mock_dev_dataset.contexts) == len(mock_dev_dataset.texts) #there is a context for every target sentence in the dev set\n",
        "assert [mock_train_dataset.texts[0], mock_train_dataset.texts[1] , mock_train_dataset.texts[2]] ==  mock_train_dataset.contexts[1] #the context for the first sentence in the corpus is only the following sentence\n",
        "assert [mock_train_dataset.texts[-2] , mock_train_dataset.texts[-1]] == mock_train_dataset.contexts[-1] #the context for the last sentence in the corpus is only the preceding sentence\n",
        "assert [mock_train_dataset.texts[mock_train_dataset.report_nos.index(1)] , mock_train_dataset.texts[mock_train_dataset.report_nos.index(1)+1]]== mock_train_dataset.contexts[mock_train_dataset.report_nos.index(1)] # first sentence of second report should have a context of only its following sentence\n",
        "assert [mock_train_dataset.texts[mock_train_dataset.report_nos.index(1)-2], mock_train_dataset.texts[mock_train_dataset.report_nos.index(1)-1]] == mock_train_dataset.contexts[mock_train_dataset.report_nos.index(1)-1] # last sentence of first report should have a context of only its preceding sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Fyf-E4gtB90"
      },
      "outputs": [],
      "source": [
        "# Unit tests -> check if assisted data labelling has been performed correctly\n",
        "assert len(set(pre_labeled_train_data['positions']).intersection(set(train_data['positions']))) == 0 \n",
        "assert (len(train_data['texts']) + len(pre_labeled_train_data['texts'])) == len(training_data)\n",
        "assert set(pre_labeled_train_data['positions']).union(set(train_data['positions'])) == set(range(len(training_data)))\n",
        "assert (len(dev_data['texts']) + len(pre_labeled_dev_data['texts'])) == len(development_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NUOfidVU_zE8"
      },
      "outputs": [],
      "source": [
        "class Sustainable_BERT(BertPreTrainedModel):\n",
        "    \"\"\" Transformer model class with custom output layer for fine-tuning.\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.bert = BertModel(config)\n",
        "        self.projection = torch.nn.Sequential(torch.nn.Dropout(0.1), torch.nn.Linear(config.hidden_size, 5))              \n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        labels=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None):\n",
        " \n",
        "        outputs = self.bert(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "        logits = self.projection(outputs.last_hidden_state) \n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O2aWTy5uAHGj"
      },
      "outputs": [],
      "source": [
        "class Trainer_Sustainable(Trainer):\n",
        "    \"\"\" Class inheriting from Trainer to configure loss function used.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "        crf_model,\n",
        "        model = None,\n",
        "        args = None,\n",
        "        data_collator = None,\n",
        "        train_dataset = None,\n",
        "        eval_dataset = None,\n",
        "        tokenizer = None,\n",
        "        model_init = None,\n",
        "        compute_metrics = None,\n",
        "        callbacks = None,\n",
        "        optimizers = (None, None),        \n",
        "        ):\n",
        "        super().__init__(model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers)\n",
        "        self.crf_model = crf_model\n",
        "\n",
        "    def compute_loss(self, model, inputs, global_target_sentence_index = global_target_sentence_index, max_paragraph_length = max_paragraph_length, return_outputs=False):\n",
        "\n",
        "        labels = inputs.pop('labels')\n",
        "        sep_positions = inputs.pop('sep_positions') # take all sep positions\n",
        "        outputs = model(**inputs)\n",
        "        batch_preds = []\n",
        "        for i, sentence_sep_positions in zip(range(outputs.shape[0]), sep_positions):\n",
        "          sentence_preds = []\n",
        "          for j in sentence_sep_positions:\n",
        "            sentence_preds.append(outputs[i,j])\n",
        "          batch_preds.append(torch.cat(sentence_preds[:-1])) #ignore last sep token as we don't predict from it\n",
        "\n",
        "        preds = torch.cat(batch_preds).reshape((-1, max_paragraph_length, 5)).permute(1,0,2)\n",
        "        labels = labels.permute(1,0)\n",
        " \n",
        "        loss = -1 * self.crf_model(preds, labels)\n",
        "        \n",
        "        if return_outputs: \n",
        "            return (loss, (loss, outputs)) \n",
        "        else:\n",
        "            return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oxRyedCktB91"
      },
      "outputs": [],
      "source": [
        "def model_predict(model, tokenizer, dataloader, device, global_target_sentence_index, max_paragraph_length, crf_model):\n",
        "    \"\"\" Utility function to set the model to GPU and infer of given dataloader.\n",
        "    \"\"\"\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            batch_sequences = []\n",
        "            batch_sep_positions = []\n",
        "            batch_token_type_ids = []\n",
        "            for sequence_list in batch['contexts']:\n",
        "              augmented_sequence = ''\n",
        "              for sentence in sequence_list:\n",
        "                  augmented_sequence += '[SEP]' + sentence\n",
        "              augmented_sequence.strip()\n",
        "              batch_sequences.append(augmented_sequence)\n",
        "            encoded_batch = tokenizer(batch_sequences, padding='longest', truncation=True, max_length=512, return_tensors='pt').to(device)\n",
        "            for encoded_sequence in encoded_batch['input_ids']:\n",
        "                sep_positions = [index for index in range(len(encoded_sequence)) if encoded_sequence[index]==102]\n",
        "                while len(sep_positions) < max_paragraph_length + 1: # repeat last sep position to get full sequence \n",
        "                  sep_positions.append(sep_positions[-1])\n",
        "                batch_sep_positions.append(sep_positions)\n",
        "                if global_target_sentence_index in range(len(sep_positions)-1): # check to see that the target sentence is actually part of the context\n",
        "                  custom_token_type_ids = [ 1 if index in range(sep_positions[global_target_sentence_index], sep_positions[global_target_sentence_index+1]+1) else 0 for index in range(len(encoded_sequence))]\n",
        "                else:\n",
        "                  custom_token_type_ids = [1 for index in range(len(encoded_sequence))]\n",
        "                batch_token_type_ids.append(custom_token_type_ids)\n",
        "            \n",
        "            encoded_batch['token_type_ids'] = torch.tensor(batch_token_type_ids).to(device)\n",
        "            output = model(**encoded_batch) \n",
        "            batch_preds = []\n",
        "            \n",
        "            for i, sentence_sep_positions in zip(range(output.shape[0]), batch_sep_positions):\n",
        "              sentence_preds = []\n",
        "              for j in sentence_sep_positions:\n",
        "                sentence_preds.append(output[i,j])\n",
        "              batch_preds.append(torch.cat(sentence_preds[:-1])) #ignore last sep token as we don't predict from it\n",
        "\n",
        "            preds = torch.cat(batch_preds).reshape((-1, max_paragraph_length, 5)).permute(1,0,2)\n",
        "            predicted_tags = [tag[global_target_sentence_index] for tag in crf_model.decode(preds)]\n",
        "            predictions.extend(predicted_tags)\n",
        "    return predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "40ca90502c504cca9ebc7f5fb15eb1e2",
            "1ae8edcbcdcf475e95f61bad7899530a",
            "19a7f09a4e9a4fb2b791d4b5644dd1ba",
            "60961d290098490b8398a3159fa331e8",
            "cbbc06200d774b78b597f538937d3895",
            "6937a243b1d14bafad046ae18bd77cf3",
            "588bb783508f44d59702a74828638b83",
            "d7a49be58d3647269f0ce3bca0ce86cd",
            "b4a3fa4a364f40fea61e32382e7cdd00",
            "4dd7b818dd22496f86656434ab787912",
            "64672adbfdf746b1bba36a53f78cd36e",
            "e075cc66e719471487294da7b21d5500",
            "a075a9d5efd04414b52d5ae9eaa7c6ad",
            "5d6383aef22a4f0a9fc062cc58b3651a",
            "981fcdcfce8b4b8fb5ab70e55119e460",
            "487e56608747420082a16a2f6dcaa1ee",
            "6945a8349baf4982874b99a9322b1059",
            "4f417158e9fe427ba197849fd59b83e2",
            "857f0ff720de4213b4f1a85b6413d686",
            "b2b5da74bce74c8bb1669ccdf958d4e5",
            "496f05d3925544708a238c07dc7181a2",
            "0538e07a3fd64d5eae591d5e436a7773"
          ]
        },
        "id": "8MJmka-LAYIV",
        "outputId": "81884257-fa13-476d-a07a-4b24eb40d2a1",
        "tags": []
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "40ca90502c504cca9ebc7f5fb15eb1e2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/420M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing Sustainable_BERT: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing Sustainable_BERT from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing Sustainable_BERT from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of Sustainable_BERT were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['projection.1.bias', 'projection.1.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e075cc66e719471487294da7b21d5500",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.96k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running training *****\n",
            "  Num examples = 36920\n",
            "  Num Epochs = 10\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 2\n",
            "  Total optimization steps = 23070\n",
            "/usr/local/lib/python3.7/dist-packages/torchcrf/__init__.py:249: UserWarning: where received a uint8 condition tensor. This behavior is deprecated and will be removed in a future version of PyTorch. Use a boolean condition instead. (Triggered internally at  /pytorch/aten/src/ATen/native/TensorCompare.cpp:255.)\n",
            "  score = torch.where(mask[i].unsqueeze(1), next_score, score)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='23070' max='23070' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [23070/23070 3:09:32, Epoch 9/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>17.846300</td>\n",
              "      <td>14.524508</td>\n",
              "      <td>0.192279</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>11.985700</td>\n",
              "      <td>17.910591</td>\n",
              "      <td>0.209630</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>9.597500</td>\n",
              "      <td>18.553713</td>\n",
              "      <td>0.257413</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>8.137300</td>\n",
              "      <td>19.403378</td>\n",
              "      <td>0.269650</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>7.187000</td>\n",
              "      <td>21.352606</td>\n",
              "      <td>0.261746</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>6.295800</td>\n",
              "      <td>21.937500</td>\n",
              "      <td>0.271273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>5.238200</td>\n",
              "      <td>22.279305</td>\n",
              "      <td>0.324231</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>4.457600</td>\n",
              "      <td>24.008759</td>\n",
              "      <td>0.309300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>3.922400</td>\n",
              "      <td>25.032305</td>\n",
              "      <td>0.310560</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>3.582300</td>\n",
              "      <td>25.234215</td>\n",
              "      <td>0.319540</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 20402\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./bert_sustainable_CRF_5seq_paper/checkpoint-2307\n",
            "Configuration saved in ./bert_sustainable_CRF_5seq_paper/checkpoint-2307/config.json\n",
            "Model weights saved in ./bert_sustainable_CRF_5seq_paper/checkpoint-2307/pytorch_model.bin\n",
            "Deleting older checkpoint [bert_sustainable_CRF_5seq_paper/checkpoint-4614] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 20402\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./bert_sustainable_CRF_5seq_paper/checkpoint-4614\n",
            "Configuration saved in ./bert_sustainable_CRF_5seq_paper/checkpoint-4614/config.json\n",
            "Model weights saved in ./bert_sustainable_CRF_5seq_paper/checkpoint-4614/pytorch_model.bin\n",
            "Deleting older checkpoint [bert_sustainable_CRF_5seq_paper/checkpoint-6921] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 20402\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./bert_sustainable_CRF_5seq_paper/checkpoint-6921\n",
            "Configuration saved in ./bert_sustainable_CRF_5seq_paper/checkpoint-6921/config.json\n",
            "Model weights saved in ./bert_sustainable_CRF_5seq_paper/checkpoint-6921/pytorch_model.bin\n",
            "Deleting older checkpoint [bert_sustainable_CRF_5seq_paper/checkpoint-2307] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 20402\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./bert_sustainable_CRF_5seq_paper/checkpoint-9228\n",
            "Configuration saved in ./bert_sustainable_CRF_5seq_paper/checkpoint-9228/config.json\n",
            "Model weights saved in ./bert_sustainable_CRF_5seq_paper/checkpoint-9228/pytorch_model.bin\n",
            "Deleting older checkpoint [bert_sustainable_CRF_5seq_paper/checkpoint-4614] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 20402\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./bert_sustainable_CRF_5seq_paper/checkpoint-11535\n",
            "Configuration saved in ./bert_sustainable_CRF_5seq_paper/checkpoint-11535/config.json\n",
            "Model weights saved in ./bert_sustainable_CRF_5seq_paper/checkpoint-11535/pytorch_model.bin\n",
            "Deleting older checkpoint [bert_sustainable_CRF_5seq_paper/checkpoint-6921] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 20402\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./bert_sustainable_CRF_5seq_paper/checkpoint-13842\n",
            "Configuration saved in ./bert_sustainable_CRF_5seq_paper/checkpoint-13842/config.json\n",
            "Model weights saved in ./bert_sustainable_CRF_5seq_paper/checkpoint-13842/pytorch_model.bin\n",
            "Deleting older checkpoint [bert_sustainable_CRF_5seq_paper/checkpoint-9228] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 20402\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./bert_sustainable_CRF_5seq_paper/checkpoint-16149\n",
            "Configuration saved in ./bert_sustainable_CRF_5seq_paper/checkpoint-16149/config.json\n",
            "Model weights saved in ./bert_sustainable_CRF_5seq_paper/checkpoint-16149/pytorch_model.bin\n",
            "Deleting older checkpoint [bert_sustainable_CRF_5seq_paper/checkpoint-11535] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 20402\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./bert_sustainable_CRF_5seq_paper/checkpoint-18456\n",
            "Configuration saved in ./bert_sustainable_CRF_5seq_paper/checkpoint-18456/config.json\n",
            "Model weights saved in ./bert_sustainable_CRF_5seq_paper/checkpoint-18456/pytorch_model.bin\n",
            "Deleting older checkpoint [bert_sustainable_CRF_5seq_paper/checkpoint-13842] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 20402\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./bert_sustainable_CRF_5seq_paper/checkpoint-20763\n",
            "Configuration saved in ./bert_sustainable_CRF_5seq_paper/checkpoint-20763/config.json\n",
            "Model weights saved in ./bert_sustainable_CRF_5seq_paper/checkpoint-20763/pytorch_model.bin\n",
            "Deleting older checkpoint [bert_sustainable_CRF_5seq_paper/checkpoint-18456] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 20402\n",
            "  Batch size = 8\n",
            "Saving model checkpoint to ./bert_sustainable_CRF_5seq_paper/checkpoint-23070\n",
            "Configuration saved in ./bert_sustainable_CRF_5seq_paper/checkpoint-23070/config.json\n",
            "Model weights saved in ./bert_sustainable_CRF_5seq_paper/checkpoint-23070/pytorch_model.bin\n",
            "Deleting older checkpoint [bert_sustainable_CRF_5seq_paper/checkpoint-20763] due to args.save_total_limit\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from ./bert_sustainable_CRF_5seq_paper/checkpoint-16149 (score: 0.3242305767529537).\n",
            "Saving model checkpoint to ./final_bert_sustainable_CRF_5seq_paper\n",
            "Configuration saved in ./final_bert_sustainable_CRF_5seq_paper/config.json\n",
            "Model weights saved in ./final_bert_sustainable_CRF_5seq_paper/pytorch_model.bin\n"
          ]
        }
      ],
      "source": [
        "# Instantiate and train model\n",
        "model = Sustainable_BERT.from_pretrained('bert-base-uncased').to(device) \n",
        "\n",
        "total_epochs = 10   \n",
        "learning_rate = 1e-5 \n",
        "\n",
        "# Create evaluation metric F1 score \n",
        "metric = load_metric('f1')\n",
        "\n",
        "def compute_metrics(eval_pred, sep_positions = dev_sep_positions, global_target_sentence_index = global_target_sentence_index):\n",
        "    raw_predictions, raw_labels = eval_pred \n",
        "    pooled_labels = [label[global_target_sentence_index] for label in raw_labels]\n",
        "    pooled_predictions = []\n",
        "    for i, sentence_sep_positions in zip(range(len(raw_predictions)), sep_positions):\n",
        "        sentence_preds = []\n",
        "        for j in sentence_sep_positions:\n",
        "            sentence_preds.append(torch.tensor(raw_predictions[i,j]))\n",
        "        pred = torch.cat(sentence_preds).reshape((-1, max_paragraph_length, 5)).permute(1,0,2).to(device)\n",
        "        predicted_tag = trainer.crf_model.decode(pred)[0][global_target_sentence_index] \n",
        "        pooled_predictions.append(predicted_tag)   \n",
        "    torch.save(trainer.crf_model, f='crf_model_IOBES_5seq_paper.pt')  # save trained crf model to use at inference time\n",
        "    return metric.compute(predictions=pooled_predictions, references=pooled_labels, average = 'macro')\n",
        "\n",
        "# Define optimizer and lr schedule\n",
        "optimizer = transformers.AdamW(model.parameters(),\n",
        "                  lr = learning_rate, \n",
        "                  )\n",
        "\n",
        "total_steps = len(train_loader) * total_epochs \n",
        "warmup = 0.06 * total_steps\n",
        " \n",
        "# Create the learning rate scheduler.\n",
        "scheduler = transformers.get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = warmup, \n",
        "                                            num_training_steps = total_steps)\n",
        "\n",
        "# Create training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./bert_sustainable_CRF_5seq_paper',\n",
        "    save_total_limit = 2,\n",
        "    learning_rate = learning_rate, \n",
        "    logging_strategy = 'epoch',\n",
        "    per_device_train_batch_size=8, \n",
        "    num_train_epochs = total_epochs, \n",
        "    save_strategy = 'epoch',\n",
        "    load_best_model_at_end = True,\n",
        "    do_eval = True,\n",
        "    evaluation_strategy = 'epoch',\n",
        "    metric_for_best_model = 'f1',\n",
        "    eval_accumulation_steps=0.1*len(dev_loader),\n",
        "    gradient_accumulation_steps = 2, # effective training batch size of 16\n",
        "    )\n",
        "\n",
        "# Define trainer module\n",
        "trainer = Trainer_Sustainable(\n",
        "    model=model,                         \n",
        "    args=training_args,                 \n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=dev_dataset,                   \n",
        "    data_collator=train_dataset.collate_fn,\n",
        "    callbacks =[transformers.EarlyStoppingCallback(early_stopping_patience = 5, early_stopping_threshold=-0.03)],\n",
        "    compute_metrics = compute_metrics,\n",
        "    optimizers = (optimizer, scheduler),\n",
        "    crf_model = CRF(num_tags=5).to(device),\n",
        "    )\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "trainer.save_model('./final_bert_sustainable_CRF_5seq_paper')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "nqpGGvvisoJU"
      },
      "outputs": [],
      "source": [
        "def reconcile_mapping(model_data, pre_labeled_data, model_predictions):\n",
        "    \"\"\" Utility function to reconcile mapping between pre-labeled data and model predictions.\n",
        "    params: predictions: list of model predictions\n",
        "            pre_labeled_data: dictionary outputted from reader function\n",
        "    returns: pred_mapping: dict\n",
        "             predictions: dict\n",
        "    \"\"\"\n",
        "    pred_mapping = {}\n",
        "    for dataset_text, text_position, prediction in zip(model_data['texts'], model_data['positions'], model_predictions):\n",
        "        pred_mapping[text_position] = (dataset_text, prediction)\n",
        "\n",
        "    pre_labeled_mapping = {}\n",
        "    for text, pos, label in zip(pre_labeled_data['texts'], pre_labeled_data['positions'], pre_labeled_data['labels']):\n",
        "        pre_labeled_mapping[pos] = (text, label)\n",
        "\n",
        "\n",
        "    pred_mapping.update(pre_labeled_mapping)\n",
        "\n",
        "    pred_mapping = {k: v for k, v in sorted(pred_mapping.items(), key=lambda item: item[0])}\n",
        "\n",
        "    predictions =[element[1] for element in list(pred_mapping.values())] \n",
        "\n",
        "    return pred_mapping, predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "5i2N8mqKULGh",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Create predictions dictionary spanning initiatives\n",
        "def sentence_to_initiative_aggregation(predictions, predictions_report_numbers):\n",
        "    \"\"\" Utility function which takes in a list of IOBES predictions per sentence and aggregates these into a dictionary of initiatives.\n",
        "    params: predictions: list of multi-class predictions\n",
        "    returns: predictions_dict: {initiative_number_report_number:list of sentence positions}\n",
        "    \"\"\"\n",
        "    predictions_dict = {}\n",
        "    initiative_index = 0\n",
        "    prediction_index = 0\n",
        "    while prediction_index < len(predictions):\n",
        "      if predictions[prediction_index] == 0: #no initiative\n",
        "        prediction_index += 1\n",
        "      elif predictions[prediction_index] == 1: #singleton\n",
        "        prediction_span = [prediction_index]\n",
        "        predictions_dict[str(initiative_index)+'_'+str(predictions_report_numbers[prediction_index])] = prediction_span\n",
        "        initiative_index += 1\n",
        "        prediction_index += 1\n",
        "      elif predictions[prediction_index] == 2: #beginning of initiative\n",
        "        if predictions[prediction_index + 1] == 4: # 2 sentence initiative\n",
        "          prediction_span = [prediction_index, prediction_index + 1]\n",
        "          predictions_dict[str(initiative_index)+'_'+str(predictions_report_numbers[prediction_index])] = prediction_span\n",
        "          initiative_index += 1\n",
        "          prediction_index += 2\n",
        "        elif (predictions[prediction_index + 1] == 3) and (predictions[prediction_index + 2] == 4): #3 sentence initiative\n",
        "          prediction_span = [prediction_index, prediction_index + 1, prediction_index + 2]\n",
        "          predictions_dict[str(initiative_index)+'_'+str(predictions_report_numbers[prediction_index])] = prediction_span\n",
        "          initiative_index += 1\n",
        "          prediction_index += 3\n",
        "        elif (predictions[prediction_index + 1] == 3) and (predictions[prediction_index + 2] == 3) and (predictions[prediction_index + 3] == 4): #4 sentence initiative\n",
        "          prediction_span = [prediction_index, prediction_index + 1, prediction_index + 2, prediction_index + 3]\n",
        "          predictions_dict[str(initiative_index)+'_'+str(predictions_report_numbers[prediction_index])] = prediction_span\n",
        "          initiative_index += 1\n",
        "          prediction_index += 4\n",
        "        elif (predictions[prediction_index + 1] == 3) and (predictions[prediction_index + 2] == 3) and (predictions[prediction_index + 3] == 3) and (predictions[prediction_index + 4] == 4): #5 sentence initiative\n",
        "          prediction_span = [prediction_index, prediction_index + 1, prediction_index + 2, prediction_index + 3, prediction_index + 3]\n",
        "          predictions_dict[str(initiative_index)+'_'+str(predictions_report_numbers[prediction_index])] = prediction_span\n",
        "          initiative_index += 1\n",
        "          prediction_index += 5\n",
        "        else:\n",
        "          prediction_span = [prediction_index]\n",
        "          predictions_dict[str(initiative_index)+'_'+str(predictions_report_numbers[prediction_index])] = prediction_span\n",
        "          initiative_index += 1\n",
        "          prediction_index += 1\n",
        "      else: # all other initiative predictions which do not form a complete BIE structure are labeled as individual singletons\n",
        "        prediction_span = [prediction_index]\n",
        "        predictions_dict[str(initiative_index)+'_'+str(predictions_report_numbers[prediction_index])] = prediction_span\n",
        "        initiative_index += 1\n",
        "        prediction_index += 1\n",
        "\n",
        "    return predictions_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "kncs_eEhsoJV"
      },
      "outputs": [],
      "source": [
        "class Initiative_Evaluation():\n",
        "    \"\"\" Class used to evaluate what % of initiatives have been correctly indentified.\n",
        "    \"\"\"\n",
        "    def __init__(self, initiative_dict, predictions_dict):\n",
        "        self.initiative_dict = initiative_dict\n",
        "        self.predictions_dict = predictions_dict\n",
        "        self.no_initiatives = len(self.initiative_dict)\n",
        "    \n",
        "    def evaluate(self):\n",
        "        if len(self.initiative_dict) == len(self.predictions_dict) == 0:\n",
        "            fully_correctly_labeled_proportion =  1\n",
        "            half_correctly_labeled_proportion = 1\n",
        "            min_correctly_labeled_proportion = 1\n",
        "            fully_correct_F1 = 1\n",
        "            half_correct_F1 = 1\n",
        "            min_correct_F1 = 1\n",
        "            return fully_correctly_labeled_proportion, half_correctly_labeled_proportion, min_correctly_labeled_proportion, fully_correct_F1, half_correct_F1, min_correct_F1\n",
        "        else:\n",
        "            # initiatize counters for true positive predictions\n",
        "            fully_correct_TP = 0\n",
        "            half_correct_TP = 0\n",
        "            min_correct_TP = 0\n",
        "            \n",
        "            # initialize lists which contain prediction IDs\\\n",
        "            #  for the first correct prediction encountered across all initatives\n",
        "            fully_correct_double_count = []\n",
        "            half_correct_double_count = []\n",
        "            min_correct_double_count = []\n",
        "\n",
        "            for initiative_ID, initiative_positions_list in self.initiative_dict.items():\n",
        "                # Keep a record of the first prediction id considered to be a success for each initiative\n",
        "                fully_correct_match_pred_ID = []\n",
        "                half_correct_match_pred_ID = []\n",
        "                min_correct_match_pred_ID = []\n",
        "                for prediction_ID, prediction_positions_list in self.predictions_dict.items():\n",
        "                    if set(initiative_positions_list).intersection(prediction_positions_list): #check if the initiative span overlaps with the predicted span\n",
        "                        if (len(set(initiative_positions_list).intersection(prediction_positions_list))/len(initiative_positions_list) == 1)\\\n",
        "                            and (len(set(prediction_positions_list).intersection(initiative_positions_list))/len(prediction_positions_list) == 1):\n",
        "                                if (len(fully_correct_match_pred_ID) == 0) and (prediction_ID not in fully_correct_double_count): \n",
        "                                    fully_correct_match_pred_ID.append(prediction_ID)\n",
        "                                    fully_correct_TP += 1\n",
        "                        if(len(set(initiative_positions_list).intersection(prediction_positions_list))/len(initiative_positions_list) >= 0.5)\\\n",
        "                            and (len(set(prediction_positions_list).intersection(initiative_positions_list))/len(prediction_positions_list) >= 0.5):\n",
        "                                if (len(half_correct_match_pred_ID) == 0) and (prediction_ID not in half_correct_double_count):\n",
        "                                    half_correct_match_pred_ID.append(prediction_ID)\n",
        "                                    half_correct_TP += 1\n",
        "                        if(len(set(initiative_positions_list).intersection(prediction_positions_list))/len(initiative_positions_list) > 0)\\\n",
        "                            and (len(set(prediction_positions_list).intersection(initiative_positions_list))/len(prediction_positions_list) > 0):\n",
        "                                if (len(min_correct_match_pred_ID) == 0) and (prediction_ID not in min_correct_double_count): \n",
        "                                        min_correct_match_pred_ID.append(prediction_ID)\n",
        "                                        min_correct_TP += 1\n",
        "                fully_correct_double_count.extend(fully_correct_match_pred_ID)\n",
        "                half_correct_double_count.extend(half_correct_match_pred_ID)\n",
        "                min_correct_double_count.extend(min_correct_match_pred_ID)\n",
        "                        \n",
        "\n",
        "            fully_correct_FN, fully_correct_FP = self.compute_FN_FP(fully_correct_TP)\n",
        "            fully_correct_F1, fully_correct_precision, fully_correct_recall = self.compute_F1(fully_correct_TP, fully_correct_FP, fully_correct_FN)\n",
        "\n",
        "            half_correct_FN, half_correct_FP = self.compute_FN_FP(half_correct_TP)\n",
        "            half_correct_F1, half_correct_precision, half_correct_recall = self.compute_F1(half_correct_TP, half_correct_FP, half_correct_FN)\n",
        "\n",
        "            min_correct_FN, min_correct_FP = self.compute_FN_FP(min_correct_TP)\n",
        "            min_correct_F1, min_correct_precision, min_correct_recall = self.compute_F1(min_correct_TP, min_correct_FP, min_correct_FN)\n",
        "\n",
        "            fully_correctly_labeled_proportion = fully_correct_TP/self.no_initiatives\n",
        "            half_correctly_labeled_proportion = half_correct_TP/self.no_initiatives\n",
        "            min_correctly_labeled_proportion = min_correct_TP/self.no_initiatives\n",
        "            \n",
        "            return fully_correctly_labeled_proportion, half_correctly_labeled_proportion, min_correctly_labeled_proportion, fully_correct_F1, half_correct_F1, min_correct_F1, fully_correct_precision, fully_correct_recall, half_correct_precision, half_correct_recall, min_correct_precision, min_correct_recall\n",
        "    \n",
        "    def compute_F1(self, TP, FP, FN):\n",
        "        \"\"\" Utility method to compute F1 score\n",
        "        \"\"\"\n",
        "        precision = TP / (TP + FP)\n",
        "        recall = TP / (TP + FN)\n",
        "        if precision == recall == 0:\n",
        "            F1 = 0\n",
        "        else:\n",
        "            F1 = 2 * precision * recall /(precision + recall)\n",
        "        return F1, precision, recall\n",
        "    \n",
        "    def compute_FN_FP(self, TP):\n",
        "        \"\"\" Utility method to compute FN and FP initiatives given the no of TP \n",
        "        (defined as the set intersection between gold initiative span and prediction span)\n",
        "        \"\"\"\n",
        "        FN = len(self.initiative_dict) - TP\n",
        "        FP = len(self.predictions_dict) - TP\n",
        "        return FN, FP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "7R7dTM4hULGj"
      },
      "outputs": [],
      "source": [
        "# Unit tests for Initiative_Evaluation Class\n",
        "mock_initiative_dict_1 = {1:[1,2], 2:[3,4]}\n",
        "mock_predictions_dict_1 = {1: [1,2], 2:[3], 3:[4]}\n",
        "mock_evaluation_1 = Initiative_Evaluation(mock_initiative_dict_1, mock_predictions_dict_1)\n",
        "mock_init_strict_accuracy_1, mock_init_medium_accuracy_1, mock_init_lenient_accuracy_1, mock1_fully_correct_F1, mock1_half_correct_F1, mock1_min_correct_F1,_,_,_,_,_,_ = mock_evaluation_1.evaluate()\n",
        "assert mock_init_strict_accuracy_1 == 0.5\n",
        "assert mock_init_medium_accuracy_1 == 1\n",
        "assert mock_init_lenient_accuracy_1 == 1\n",
        "assert mock1_fully_correct_F1 == 0.4\n",
        "assert mock1_half_correct_F1 == mock1_min_correct_F1 == 0.8\n",
        "\n",
        "\n",
        "mock_initiative_dict_2 = {1:[1,2], 2:[4,5,6]}\n",
        "mock_predictions_dict_2 = {1: [1,2], 2:[4,5,6]}\n",
        "mock_evaluation_2 = Initiative_Evaluation(mock_initiative_dict_2, mock_predictions_dict_2)\n",
        "mock_init_strict_accuracy_2, mock_init_medium_accuracy_2, mock_init_lenient_accuracy_2, mock2_fully_correct_F1, mock2_half_correct_F1, mock2_min_correct_F1,_,_,_,_,_,_ = mock_evaluation_2.evaluate()\n",
        "assert mock_init_strict_accuracy_2 == 1\n",
        "assert mock_init_medium_accuracy_2 == 1\n",
        "assert mock_init_lenient_accuracy_2 == 1\n",
        "assert mock2_fully_correct_F1 == mock2_half_correct_F1 == mock2_min_correct_F1 == 1\n",
        "\n",
        "mock_initiative_dict_3 = {1:[1,2], 2:[3,4]}\n",
        "mock_predictions_dict_3 = {1: [1,2], 2:[3,4]}\n",
        "mock_evaluation_3 = Initiative_Evaluation(mock_initiative_dict_3, mock_predictions_dict_3)\n",
        "mock_init_strict_accuracy_3, mock_init_medium_accuracy_3, mock_init_lenient_accuracy_3, mock3_fully_correct_F1, mock3_half_correct_F1, mock3_min_correct_F1,_,_,_,_,_,_ = mock_evaluation_3.evaluate()\n",
        "assert mock_init_strict_accuracy_3 == 1\n",
        "assert mock_init_medium_accuracy_3 == 1\n",
        "assert mock_init_lenient_accuracy_3 == 1\n",
        "assert mock3_fully_correct_F1 == mock3_half_correct_F1 == mock3_min_correct_F1 == 1\n",
        "\n",
        "mock_initiative_dict_4 = {}\n",
        "mock_predictions_dict_4 = {}\n",
        "mock_evaluation_4 = Initiative_Evaluation(mock_initiative_dict_4, mock_predictions_dict_4)\n",
        "mock_init_strict_accuracy_4, mock_init_medium_accuracy_4, mock_init_lenient_accuracy_4, mock4_fully_correct_F1, mock4_half_correct_F1, mock4_min_correct_F1 = mock_evaluation_4.evaluate()\n",
        "assert mock_init_strict_accuracy_4 == 1\n",
        "assert mock_init_medium_accuracy_4 == 1\n",
        "assert mock_init_lenient_accuracy_4 == 1\n",
        "assert mock4_fully_correct_F1 == mock4_half_correct_F1 == mock4_min_correct_F1 == 1\n",
        "\n",
        "mock_initiative_dict_5 = {1:[1,2], 2:[3,4,5], 3:[6]}\n",
        "mock_predictions_dict_5 = {1:[1], 2:[2], 3:[3], 4:[4], 5:[5]}\n",
        "mock_evaluation_5 = Initiative_Evaluation(mock_initiative_dict_5, mock_predictions_dict_5)\n",
        "mock_init_strict_accuracy_5, mock_init_medium_accuracy_5, mock_init_lenient_accuracy_5, mock5_fully_correct_F1, mock5_half_correct_F1, mock5_min_correct_F1,_,_,_,_,_,_ = mock_evaluation_5.evaluate()\n",
        "assert mock_init_strict_accuracy_5 == 0\n",
        "assert mock_init_medium_accuracy_5 == 1/3\n",
        "assert mock_init_lenient_accuracy_5 == 2/3\n",
        "assert mock5_fully_correct_F1 == 0\n",
        "assert mock5_half_correct_F1 == 0.25\n",
        "assert mock5_min_correct_F1 == 0.5\n",
        "\n",
        "mock_initiative_dict_6 = {1:[1,2], 2:[3,4,5], 3:[6]}\n",
        "mock_predictions_dict_6 = {1:[1,2,3,4,5,6]}\n",
        "mock_evaluation_6 = Initiative_Evaluation(mock_initiative_dict_6, mock_predictions_dict_6)\n",
        "mock_init_strict_accuracy_6, mock_init_medium_accuracy_6, mock_init_lenient_accuracy_6, mock6_fully_correct_F1, mock6_half_correct_F1, mock6_min_correct_F1,_,_,_,_,_,_ = mock_evaluation_6.evaluate()\n",
        "assert mock_init_strict_accuracy_6 == 0\n",
        "assert mock_init_medium_accuracy_6 == 1/3\n",
        "assert mock_init_lenient_accuracy_6 == 1/3\n",
        "assert mock6_fully_correct_F1 == 0\n",
        "assert mock6_half_correct_F1 == mock6_min_correct_F1 == 0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CM5U12qZtB92",
        "outputId": "de00af49-d16a-45c4-cea3-d204c0b43c43"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading configuration file ./final_bert_sustainable_CRF_5seq_paper/config.json\n",
            "Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-uncased\",\n",
            "  \"architectures\": [\n",
            "    \"Sustainable_BERT\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.11.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file ./final_bert_sustainable_CRF_5seq_paper/pytorch_model.bin\n",
            "All model checkpoint weights were used when initializing Sustainable_BERT.\n",
            "\n",
            "All the weights of Sustainable_BERT were initialized from the model checkpoint at ./final_bert_sustainable_CRF_5seq_paper/.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use Sustainable_BERT for predictions without further training.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicting results on dev set took 275.13120007514954 seconds\n"
          ]
        }
      ],
      "source": [
        "# Perform context predictions on dev dataset\n",
        "start_time = time.time()\n",
        "sustainable_model = Sustainable_BERT.from_pretrained('./final_bert_sustainable_CRF_5seq_paper/') \n",
        "loaded_crf_model = torch.load('crf_model_IOBES_5seq_paper.pt') \n",
        "dev_predictions_list = model_predict(sustainable_model, tokenizer, dev_loader, device, global_target_sentence_index = global_target_sentence_index, max_paragraph_length = max_paragraph_length, crf_model = loaded_crf_model)\n",
        "end_time = time.time()\n",
        "print(f'Predicting results on dev set took {end_time-start_time} seconds')\n",
        "\n",
        "# Reconcile predictions on the dev set\n",
        "dev_pred_mapping, dev_predictions = reconcile_mapping(dev_data, pre_labeled_dev_data, dev_predictions_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ioIJdeSTtB92",
        "outputId": "111b9ae7-f287-476e-fa30-42c3154b3755"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classification Report on the Development Dataset \n",
            "\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "No Initiative     0.9787    0.9904    0.9845     53854\n",
            "    Singleton     0.3487    0.1349    0.1946       504\n",
            "    Beginning     0.2440    0.2100    0.2257       481\n",
            "       Inside     0.1884    0.0419    0.0686       310\n",
            "          End     0.1769    0.1684    0.1725       481\n",
            "\n",
            "     accuracy                         0.9635     55630\n",
            "    macro avg     0.3873    0.3091    0.3292     55630\n",
            " weighted avg     0.9554    0.9635    0.9587     55630\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Extract ground truth dev data labels\n",
        "dev_label_values = []\n",
        "dev_report_numbers = []\n",
        "for sent_no in range(len(development_data)):\n",
        "  dev_report_numbers.append(development_data[sent_no][0]['report_no'])\n",
        "  if development_data[sent_no][3]['list_of_initiatives']:\n",
        "    initiative_unique_reference = development_data[sent_no][3]['list_of_initiatives'][0] + '_' + str(development_data[sent_no][0]['report_no'])\n",
        "    if len(dev_initiative_dict[initiative_unique_reference]) == 1:\n",
        "      dev_label_values.append(development_data[sent_no][2]['has_initiative']) # append 1 for singletons or 0 for non-initiative sentences\n",
        "    elif dev_initiative_dict[initiative_unique_reference].index(sent_no) == 0:\n",
        "      dev_label_values.append(2) #append 2 for beginning of initiative\n",
        "    elif dev_initiative_dict[initiative_unique_reference].index(sent_no) == (len(dev_initiative_dict[initiative_unique_reference]) - 1):\n",
        "      dev_label_values.append(4) #append 4 for end of initiative\n",
        "    else:\n",
        "      dev_label_values.append(3) #append 3 for inside an initiative\n",
        "  else:\n",
        "    dev_label_values.append(development_data[sent_no][2]['has_initiative'])\n",
        "\n",
        "\n",
        "target_names = ['No Initiative', 'Singleton', 'Beginning', 'Inside', 'End']\n",
        "print(f'Classification Report on the Development Dataset \\n')\n",
        "print(classification_report(dev_label_values, np.array(dev_predictions), target_names = target_names, digits=4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YkeXSSd8soJW",
        "outputId": "98ee37b0-4e0b-461a-bd7c-3badbf4d77b8",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Binary Classification Report on the Development Dataset \n",
            "\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "No Initiative     0.9787    0.9904    0.9845     53854\n",
            "   Initiative     0.5440    0.3480    0.4245      1776\n",
            "\n",
            "     accuracy                         0.9699     55630\n",
            "    macro avg     0.7614    0.6692    0.7045     55630\n",
            " weighted avg     0.9649    0.9699    0.9667     55630\n",
            "\n"
          ]
        }
      ],
      "source": [
        "dev_binary_predictions = [1 if prediction else 0 for prediction in dev_predictions]\n",
        "dev_label_binary_values = [1 if value else 0 for value in dev_label_values]\n",
        "print(f'Binary Classification Report on the Development Dataset \\n')\n",
        "target_names = ['No Initiative', 'Initiative']\n",
        "print(classification_report(dev_label_binary_values, np.array(dev_binary_predictions), target_names = target_names, digits=4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eyjAu4KjYC92",
        "outputId": "90993655-6bb5-43eb-ace0-a96bcf8255bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Percentage of correctly predicted initiatives where at least 1 sentence is identified is 43.35% \n",
            "\n",
            "Percentage of correctly predicted initiatives where more than 50% of sentences are identified is 36.85% \n",
            "\n",
            "Percentage of correctly predicted initiatives where 100% of sentences are identified is 20.10% \n",
            "\n",
            "F1 score where at least 1 sentence is identified is 43.73% \n",
            "\n",
            "Precision score where at least 1 sentence is identified is 44.11% \n",
            "\n",
            "Recall score where at least 1 sentence is identified is 43.35% \n",
            "\n",
            "F1 score where 50% of sentences are identified is 37.17% \n",
            "\n",
            "Precision score where 50% of sentences are identified is 37.50% \n",
            "\n",
            "Recal score where 50% of sentences are identified is 36.85% \n",
            "\n",
            "F1 score where 100% of sentences are identified is 20.28% \n",
            "\n",
            "Precision score where 100% of sentences are identified is 20.45% \n",
            "\n",
            "Recall score where 100% of sentences are identified is 20.10% \n",
            "\n"
          ]
        }
      ],
      "source": [
        "dev_predictions_dict = sentence_to_initiative_aggregation(dev_predictions, dev_report_numbers)\n",
        "dev_init_evaluation = Initiative_Evaluation(dev_initiative_dict, dev_predictions_dict)\n",
        "dev_init_strict_accuracy, dev_init_medium_accuracy, dev_init_lenient_accuracy, dev_strict_F1, dev_medium_F1, dev_lenient_F1, dev_strict_precision, dev_strict_recall, dev_medium_precision, dev_medium_recall, dev_lenient_precision, dev_lenient_recall = dev_init_evaluation.evaluate()\n",
        "\n",
        "print(f'Percentage of correctly predicted initiatives where at least 1 sentence is identified is {dev_init_lenient_accuracy:.2%} \\n')\n",
        "print(f'Percentage of correctly predicted initiatives where more than 50% of sentences are identified is {dev_init_medium_accuracy:.2%} \\n')\n",
        "print(f'Percentage of correctly predicted initiatives where 100% of sentences are identified is {dev_init_strict_accuracy:.2%} \\n')\n",
        "print(f'F1 score where at least 1 sentence is identified is {dev_lenient_F1:.2%} \\n')\n",
        "print(f'Precision score where at least 1 sentence is identified is {dev_lenient_precision:.2%} \\n')\n",
        "print(f'Recall score where at least 1 sentence is identified is {dev_lenient_recall:.2%} \\n')\n",
        "print(f'F1 score where 50% of sentences are identified is {dev_medium_F1:.2%} \\n')\n",
        "print(f'Precision score where 50% of sentences are identified is {dev_medium_precision:.2%} \\n')\n",
        "print(f'Recal score where 50% of sentences are identified is {dev_medium_recall:.2%} \\n')\n",
        "print(f'F1 score where 100% of sentences are identified is {dev_strict_F1:.2%} \\n')\n",
        "print(f'Precision score where 100% of sentences are identified is {dev_strict_precision:.2%} \\n')\n",
        "print(f'Recall score where 100% of sentences are identified is {dev_strict_recall:.2%} \\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RPk8p37REOj6",
        "outputId": "d3d64715-d4f3-4406-a332-b1b9ce02082f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicting results on test set took 286.40934896469116 seconds\n"
          ]
        }
      ],
      "source": [
        "# Perform predictions on test dataset\n",
        "start_time = time.time()\n",
        "test_predictions_list = model_predict(sustainable_model, tokenizer, test_loader, device,  global_target_sentence_index = global_target_sentence_index, max_paragraph_length = max_paragraph_length, crf_model = loaded_crf_model)\n",
        "end_time = time.time()\n",
        "print(f'Predicting results on test set took {end_time-start_time} seconds')\n",
        "\n",
        "# Reconcile predictions on the train set\n",
        "test_pred_mapping, test_predictions = reconcile_mapping(test_data, pre_labeled_test_data, test_predictions_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lt0B3gzqEOj6",
        "outputId": "3de459dd-239c-449f-81de-ca06b3997ee6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classification Report on the Test Dataset \n",
            "\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "No Initiative     0.9764    0.9803    0.9783     48175\n",
            "    Singleton     0.2518    0.1802    0.2101       577\n",
            "    Beginning     0.1674    0.1633    0.1653       447\n",
            "       Inside     0.1410    0.0472    0.0707       233\n",
            "          End     0.1336    0.1745    0.1513       447\n",
            "\n",
            "     accuracy                         0.9521     49879\n",
            "    macro avg     0.3340    0.3091    0.3152     49879\n",
            " weighted avg     0.9493    0.9521    0.9505     49879\n",
            "\n"
          ]
        }
      ],
      "source": [
        "test_label_values = []\n",
        "test_report_numbers = []\n",
        "for sent_no in range(len(testing_data)):\n",
        "  test_report_numbers.append(testing_data[sent_no][0]['report_no'])\n",
        "  if testing_data[sent_no][3]['list_of_initiatives']:\n",
        "    initiative_unique_reference = testing_data[sent_no][3]['list_of_initiatives'][0] + '_' + str(testing_data[sent_no][0]['report_no'])\n",
        "    if len(test_initiative_dict[initiative_unique_reference]) == 1:\n",
        "      test_label_values.append(testing_data[sent_no][2]['has_initiative']) # append 1 for singletons or 0 for non-initiative sentences\n",
        "    elif test_initiative_dict[initiative_unique_reference].index(sent_no) == 0:\n",
        "      test_label_values.append(2) #append 2 for beginning of initiative\n",
        "    elif test_initiative_dict[initiative_unique_reference].index(sent_no) == (len(test_initiative_dict[initiative_unique_reference]) - 1):\n",
        "      test_label_values.append(4) #append 4 for end of initiative\n",
        "    else:\n",
        "      test_label_values.append(3) #append 3 for inside an initiative\n",
        "  else:\n",
        "    test_label_values.append(testing_data[sent_no][2]['has_initiative'])\n",
        "\n",
        "\n",
        "target_names = ['No Initiative', 'Singleton', 'Beginning', 'Inside', 'End']\n",
        "print(f'Classification Report on the Test Dataset \\n')\n",
        "print(classification_report(test_label_values, np.array(test_predictions), target_names = target_names, digits=4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6zEPSxzNbzzM",
        "outputId": "dc928918-90b1-45c0-d075-c963747ba5fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Binary Classification Report on the Test Dataset \n",
            "\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "No Initiative     0.9764    0.9803    0.9783     48175\n",
            "   Initiative     0.3719    0.3298    0.3496      1704\n",
            "\n",
            "     accuracy                         0.9581     49879\n",
            "    macro avg     0.6742    0.6551    0.6640     49879\n",
            " weighted avg     0.9557    0.9581    0.9569     49879\n",
            "\n"
          ]
        }
      ],
      "source": [
        "test_binary_predictions = [1 if prediction else 0 for prediction in test_predictions]\n",
        "test_label_binary_values = [1 if value else 0 for value in test_label_values]\n",
        "print(f'Binary Classification Report on the Test Dataset \\n')\n",
        "target_names = ['No Initiative', 'Initiative']\n",
        "print(classification_report(test_label_binary_values, np.array(test_binary_predictions), target_names = target_names, digits=4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZctnRt7KEOj6",
        "outputId": "ad61e7b1-6ff6-4221-b0ea-198b5fb58f63"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Percentage of correctly predicted initiatives where at least 1 sentence is identified is 39.55% \n",
            "\n",
            "Percentage of correctly predicted initiatives where more than 50% of sentences are identified is 33.59% \n",
            "\n",
            "Percentage of correctly predicted initiatives where 100% of sentences are identified is 19.34% \n",
            "\n",
            "F1 score where at least 1 sentence is identified is 34.18% \n",
            "\n",
            "Precision score where at least 1 sentence is identified is 30.09% \n",
            "\n",
            "Recall score where at least 1 sentence is identified is 39.55% \n",
            "\n",
            "F1 score where 50% of sentences are identified is 29.03% \n",
            "\n",
            "Precision score where 50% of sentences are identified is 25.56% \n",
            "\n",
            "Recal score where 50% of sentences are identified is 33.59% \n",
            "\n",
            "F1 score where 100% of sentences are identified is 16.71% \n",
            "\n",
            "Precision score where 100% of sentences are identified is 14.71% \n",
            "\n",
            "Recall score where 100% of sentences are identified is 19.34% \n",
            "\n"
          ]
        }
      ],
      "source": [
        "test_predictions_dict = sentence_to_initiative_aggregation(test_predictions, test_report_numbers)\n",
        "test_init_evaluation = Initiative_Evaluation(test_initiative_dict, test_predictions_dict)\n",
        "test_init_strict_accuracy, test_init_medium_accuracy, test_init_lenient_accuracy, test_strict_F1, test_medium_F1, test_lenient_F1, test_strict_precision, test_strict_recall, test_medium_precision, test_medium_recall, test_lenient_precision, test_lenient_recall  = test_init_evaluation.evaluate()\n",
        "\n",
        "print(f'Percentage of correctly predicted initiatives where at least 1 sentence is identified is {test_init_lenient_accuracy:.2%} \\n')\n",
        "print(f'Percentage of correctly predicted initiatives where more than 50% of sentences are identified is {test_init_medium_accuracy:.2%} \\n')\n",
        "print(f'Percentage of correctly predicted initiatives where 100% of sentences are identified is {test_init_strict_accuracy:.2%} \\n')\n",
        "print(f'F1 score where at least 1 sentence is identified is {test_lenient_F1:.2%} \\n')\n",
        "print(f'Precision score where at least 1 sentence is identified is {test_lenient_precision:.2%} \\n')\n",
        "print(f'Recall score where at least 1 sentence is identified is {test_lenient_recall:.2%} \\n')\n",
        "print(f'F1 score where 50% of sentences are identified is {test_medium_F1:.2%} \\n')\n",
        "print(f'Precision score where 50% of sentences are identified is {test_medium_precision:.2%} \\n')\n",
        "print(f'Recal score where 50% of sentences are identified is {test_medium_recall:.2%} \\n')\n",
        "print(f'F1 score where 100% of sentences are identified is {test_strict_F1:.2%} \\n')\n",
        "print(f'Precision score where 100% of sentences are identified is {test_strict_precision:.2%} \\n')\n",
        "print(f'Recall score where 100% of sentences are identified is {test_strict_recall:.2%} \\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qf2aN65ztB92",
        "outputId": "6184ca3b-950f-4935-b506-75f789989ed7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicting results on train set took 476.2395040988922 seconds\n"
          ]
        }
      ],
      "source": [
        "# Perform predictions on train dataset\n",
        "start_time = time.time()\n",
        "train_predictions_list = model_predict(sustainable_model, tokenizer, train_loader, device,  global_target_sentence_index = global_target_sentence_index, max_paragraph_length = max_paragraph_length, crf_model = loaded_crf_model)\n",
        "end_time = time.time()\n",
        "print(f'Predicting results on train set took {end_time-start_time} seconds')\n",
        "\n",
        "# Reconcile predictions on the train set\n",
        "train_pred_mapping, train_predictions = reconcile_mapping(train_data, pre_labeled_train_data, train_predictions_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "daVKMN0stB93",
        "outputId": "239e8729-3ae7-40ea-8835-5721927da2b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classification Report on the Training Dataset \n",
            "\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "No Initiative     0.9983    0.9870    0.9926     83801\n",
            "    Singleton     0.6730    0.9100    0.7738      1045\n",
            "    Beginning     0.6603    0.7678    0.7100       995\n",
            "       Inside     0.6598    0.9621    0.7828       633\n",
            "          End     0.7032    0.7930    0.7454       995\n",
            "\n",
            "     accuracy                         0.9812     87469\n",
            "    macro avg     0.7389    0.8840    0.8009     87469\n",
            " weighted avg     0.9847    0.9812    0.9825     87469\n",
            "\n"
          ]
        }
      ],
      "source": [
        "training_labels = []\n",
        "train_report_numbers = []\n",
        "for sent_no in range(len(training_data)):\n",
        "  train_report_numbers.append(training_data[sent_no][0]['report_no'])\n",
        "  if training_data[sent_no][3]['list_of_initiatives']:\n",
        "    initiative_unique_reference = training_data[sent_no][3]['list_of_initiatives'][0] + '_' + str(training_data[sent_no][0]['report_no'])\n",
        "    if len(train_initiative_dict[initiative_unique_reference]) == 1:\n",
        "      training_labels.append(training_data[sent_no][2]['has_initiative']) # append 1 for singletons or 0 for non-initiative sentences\n",
        "    elif train_initiative_dict[initiative_unique_reference].index(sent_no) == 0:\n",
        "      training_labels.append(2) #append 2 for beginning of initiative\n",
        "    elif train_initiative_dict[initiative_unique_reference].index(sent_no) == (len(train_initiative_dict[initiative_unique_reference]) - 1):\n",
        "      training_labels.append(4) #append 4 for end of initiative\n",
        "    else:\n",
        "      training_labels.append(3) #append 3 for inside an initiative\n",
        "  else:\n",
        "    training_labels.append(training_data[sent_no][2]['has_initiative'])\n",
        "    \n",
        "target_names = ['No Initiative', 'Singleton', 'Beginning', 'Inside', 'End']\n",
        "print(f'Classification Report on the Training Dataset \\n')\n",
        "print(classification_report(training_labels, np.array(train_predictions), target_names = target_names, digits = 4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XY5BwgnLsoJX",
        "outputId": "b95be393-5d8b-46b3-ec6c-e05e87a576c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Percentage of correctly predicted initiatives where at least 1 sentence is identified is 96.76% \n",
            "\n",
            "Percentage of correctly predicted initiatives where more than 50% of sentences are identified is 85.88% \n",
            "\n",
            "Percentage of correctly predicted initiatives where 100% of sentences are identified is 72.55% \n",
            "\n",
            "F1 score where at least 1 sentence is identified is 68.19% \n",
            "\n",
            "F1 score where 50% of sentences are identified is 60.52% \n",
            "\n",
            "F1 score where 100% of sentences are identified is 51.12% \n",
            "\n",
            "Evaluating initiatives on the train set took 2.879420042037964 seconds\n"
          ]
        }
      ],
      "source": [
        "start_time = time.time()\n",
        "train_predictions_dict = sentence_to_initiative_aggregation(train_predictions, train_report_numbers)\n",
        "train_init_evaluation = Initiative_Evaluation(train_initiative_dict, train_predictions_dict)\n",
        "train_init_strict_accuracy, train_init_medium_accuracy, train_init_lenient_accuracy, train_strict_F1, train_medium_F1, train_lenient_F1, train_strict_precision, train_strict_recall, train_medium_precision, train_medium_recall, train_lenient_precision, train_lenient_recall  = train_init_evaluation.evaluate()\n",
        "\n",
        "print(f'Percentage of correctly predicted initiatives where at least 1 sentence is identified is {train_init_lenient_accuracy:.2%} \\n')\n",
        "print(f'Percentage of correctly predicted initiatives where more than 50% of sentences are identified is {train_init_medium_accuracy:.2%} \\n')\n",
        "print(f'Percentage of correctly predicted initiatives where 100% of sentences are identified is {train_init_strict_accuracy:.2%} \\n')\n",
        "print(f'F1 score where at least 1 sentence is identified is {train_lenient_F1:.2%} \\n')\n",
        "print(f'F1 score where 50% of sentences are identified is {train_medium_F1:.2%} \\n')\n",
        "print(f'F1 score where 100% of sentences are identified is {train_strict_F1:.2%} \\n')\n",
        "end_time = time.time()\n",
        "print(f'Evaluating initiatives on the train set took {end_time-start_time} seconds')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "BERT_IOBES_2_sent_context_paper.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "83822c0cf896608929095f1043a98f7e1bb9e3e58b660cf90b3c0a24621313f4"
    },
    "kernelspec": {
      "display_name": "Python 3.8.5 64-bit ('sustainability': conda)",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0538e07a3fd64d5eae591d5e436a7773": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "19a7f09a4e9a4fb2b791d4b5644dd1ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_588bb783508f44d59702a74828638b83",
            "placeholder": "​",
            "style": "IPY_MODEL_6937a243b1d14bafad046ae18bd77cf3",
            "value": "Downloading: 100%"
          }
        },
        "1ae8edcbcdcf475e95f61bad7899530a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "40ca90502c504cca9ebc7f5fb15eb1e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_19a7f09a4e9a4fb2b791d4b5644dd1ba",
              "IPY_MODEL_60961d290098490b8398a3159fa331e8",
              "IPY_MODEL_cbbc06200d774b78b597f538937d3895"
            ],
            "layout": "IPY_MODEL_1ae8edcbcdcf475e95f61bad7899530a"
          }
        },
        "487e56608747420082a16a2f6dcaa1ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0538e07a3fd64d5eae591d5e436a7773",
            "placeholder": "​",
            "style": "IPY_MODEL_496f05d3925544708a238c07dc7181a2",
            "value": " 4.64k/? [00:00&lt;00:00, 199kB/s]"
          }
        },
        "496f05d3925544708a238c07dc7181a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4dd7b818dd22496f86656434ab787912": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4f417158e9fe427ba197849fd59b83e2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "588bb783508f44d59702a74828638b83": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5d6383aef22a4f0a9fc062cc58b3651a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4f417158e9fe427ba197849fd59b83e2",
            "placeholder": "​",
            "style": "IPY_MODEL_6945a8349baf4982874b99a9322b1059",
            "value": "Downloading: "
          }
        },
        "60961d290098490b8398a3159fa331e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b4a3fa4a364f40fea61e32382e7cdd00",
            "max": 440473133,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d7a49be58d3647269f0ce3bca0ce86cd",
            "value": 440473133
          }
        },
        "64672adbfdf746b1bba36a53f78cd36e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6937a243b1d14bafad046ae18bd77cf3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6945a8349baf4982874b99a9322b1059": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "857f0ff720de4213b4f1a85b6413d686": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "981fcdcfce8b4b8fb5ab70e55119e460": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b2b5da74bce74c8bb1669ccdf958d4e5",
            "max": 1957,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_857f0ff720de4213b4f1a85b6413d686",
            "value": 1957
          }
        },
        "a075a9d5efd04414b52d5ae9eaa7c6ad": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b2b5da74bce74c8bb1669ccdf958d4e5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b4a3fa4a364f40fea61e32382e7cdd00": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cbbc06200d774b78b597f538937d3895": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_64672adbfdf746b1bba36a53f78cd36e",
            "placeholder": "​",
            "style": "IPY_MODEL_4dd7b818dd22496f86656434ab787912",
            "value": " 420M/420M [00:07&lt;00:00, 62.8MB/s]"
          }
        },
        "d7a49be58d3647269f0ce3bca0ce86cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e075cc66e719471487294da7b21d5500": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5d6383aef22a4f0a9fc062cc58b3651a",
              "IPY_MODEL_981fcdcfce8b4b8fb5ab70e55119e460",
              "IPY_MODEL_487e56608747420082a16a2f6dcaa1ee"
            ],
            "layout": "IPY_MODEL_a075a9d5efd04414b52d5ae9eaa7c6ad"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
